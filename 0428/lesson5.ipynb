{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from imp import reload \n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'data/imdb/models/'\n",
    "%mkdir -p $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 13s 8us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584,\n",
       " 1,\n",
       " 2,\n",
       " {'fawn': 34701,\n",
       "  'tsukino': 52006,\n",
       "  'nunnery': 52007,\n",
       "  'sonja': 16816,\n",
       "  'vani': 63951,\n",
       "  'woods': 1408,\n",
       "  'spiders': 16115,\n",
       "  'hanging': 2345,\n",
       "  'woody': 2289,\n",
       "  'trawling': 52008,\n",
       "  \"hold's\": 52009,\n",
       "  'comically': 11307,\n",
       "  'localized': 40830,\n",
       "  'disobeying': 30568,\n",
       "  \"'royale\": 52010,\n",
       "  \"harpo's\": 40831,\n",
       "  'canet': 52011,\n",
       "  'aileen': 19313,\n",
       "  'acurately': 52012,\n",
       "  \"diplomat's\": 52013,\n",
       "  'rickman': 25242,\n",
       "  'arranged': 6746,\n",
       "  'rumbustious': 52014,\n",
       "  'familiarness': 52015,\n",
       "  \"spider'\": 52016,\n",
       "  'hahahah': 68804,\n",
       "  \"wood'\": 52017,\n",
       "  'transvestism': 40833,\n",
       "  \"hangin'\": 34702,\n",
       "  'bringing': 2338,\n",
       "  'seamier': 40834,\n",
       "  'wooded': 34703,\n",
       "  'bravora': 52018,\n",
       "  'grueling': 16817,\n",
       "  'wooden': 1636,\n",
       "  'wednesday': 16818,\n",
       "  \"'prix\": 52019,\n",
       "  'altagracia': 34704,\n",
       "  'circuitry': 52020,\n",
       "  'crotch': 11585,\n",
       "  'busybody': 57766,\n",
       "  \"tart'n'tangy\": 52021,\n",
       "  'burgade': 14129,\n",
       "  'thrace': 52023,\n",
       "  \"tom's\": 11038,\n",
       "  'snuggles': 52025,\n",
       "  'francesco': 29114,\n",
       "  'complainers': 52027,\n",
       "  'templarios': 52125,\n",
       "  '272': 40835,\n",
       "  '273': 52028,\n",
       "  'zaniacs': 52130,\n",
       "  '275': 34706,\n",
       "  'consenting': 27631,\n",
       "  'snuggled': 40836,\n",
       "  'inanimate': 15492,\n",
       "  'uality': 52030,\n",
       "  'bronte': 11926,\n",
       "  'errors': 4010,\n",
       "  'dialogs': 3230,\n",
       "  \"yomada's\": 52031,\n",
       "  \"madman's\": 34707,\n",
       "  'dialoge': 30585,\n",
       "  'usenet': 52033,\n",
       "  'videodrome': 40837,\n",
       "  \"kid'\": 26338,\n",
       "  'pawed': 52034,\n",
       "  \"'girlfriend'\": 30569,\n",
       "  \"'pleasure\": 52035,\n",
       "  \"'reloaded'\": 52036,\n",
       "  \"kazakos'\": 40839,\n",
       "  'rocque': 52037,\n",
       "  'mailings': 52038,\n",
       "  'brainwashed': 11927,\n",
       "  'mcanally': 16819,\n",
       "  \"tom''\": 52039,\n",
       "  'kurupt': 25243,\n",
       "  'affiliated': 21905,\n",
       "  'babaganoosh': 52040,\n",
       "  \"noe's\": 40840,\n",
       "  'quart': 40841,\n",
       "  'kids': 359,\n",
       "  'uplifting': 5034,\n",
       "  'controversy': 7093,\n",
       "  'kida': 21906,\n",
       "  'kidd': 23379,\n",
       "  \"error'\": 52041,\n",
       "  'neurologist': 52042,\n",
       "  'spotty': 18510,\n",
       "  'cobblers': 30570,\n",
       "  'projection': 9878,\n",
       "  'fastforwarding': 40842,\n",
       "  'sters': 52043,\n",
       "  \"eggar's\": 52044,\n",
       "  'etherything': 52045,\n",
       "  'gateshead': 40843,\n",
       "  'airball': 34708,\n",
       "  'unsinkable': 25244,\n",
       "  'stern': 7180,\n",
       "  \"cervi's\": 52046,\n",
       "  'dnd': 40844,\n",
       "  'dna': 11586,\n",
       "  'insecurity': 20598,\n",
       "  \"'reboot'\": 52047,\n",
       "  'trelkovsky': 11037,\n",
       "  'jaekel': 52048,\n",
       "  'sidebars': 52049,\n",
       "  \"sforza's\": 52050,\n",
       "  'distortions': 17633,\n",
       "  'mutinies': 52051,\n",
       "  'sermons': 30602,\n",
       "  '7ft': 40846,\n",
       "  'boobage': 52052,\n",
       "  \"o'bannon's\": 52053,\n",
       "  'populations': 23380,\n",
       "  'chulak': 52054,\n",
       "  'mesmerize': 27633,\n",
       "  'quinnell': 52055,\n",
       "  'yahoo': 10307,\n",
       "  'meteorologist': 52057,\n",
       "  'beswick': 42577,\n",
       "  'boorman': 15493,\n",
       "  'voicework': 40847,\n",
       "  \"ster'\": 52058,\n",
       "  'blustering': 22922,\n",
       "  'hj': 52059,\n",
       "  'intake': 27634,\n",
       "  'morally': 5621,\n",
       "  'jumbling': 40849,\n",
       "  'bowersock': 52060,\n",
       "  \"'porky's'\": 52061,\n",
       "  'gershon': 16821,\n",
       "  'ludicrosity': 40850,\n",
       "  'coprophilia': 52062,\n",
       "  'expressively': 40851,\n",
       "  \"india's\": 19500,\n",
       "  \"post's\": 34710,\n",
       "  'wana': 52063,\n",
       "  'wang': 5283,\n",
       "  'wand': 30571,\n",
       "  'wane': 25245,\n",
       "  'edgeways': 52321,\n",
       "  'titanium': 34711,\n",
       "  'pinta': 40852,\n",
       "  'want': 178,\n",
       "  'pinto': 30572,\n",
       "  'whoopdedoodles': 52065,\n",
       "  'tchaikovsky': 21908,\n",
       "  'travel': 2103,\n",
       "  \"'victory'\": 52066,\n",
       "  'copious': 11928,\n",
       "  'gouge': 22433,\n",
       "  \"chapters'\": 52067,\n",
       "  'barbra': 6702,\n",
       "  'uselessness': 30573,\n",
       "  \"wan'\": 52068,\n",
       "  'assimilated': 27635,\n",
       "  'petiot': 16116,\n",
       "  'most\\x85and': 52069,\n",
       "  'dinosaurs': 3930,\n",
       "  'wrong': 352,\n",
       "  'seda': 52070,\n",
       "  'stollen': 52071,\n",
       "  'sentencing': 34712,\n",
       "  'ouroboros': 40853,\n",
       "  'assimilates': 40854,\n",
       "  'colorfully': 40855,\n",
       "  'glenne': 27636,\n",
       "  'dongen': 52072,\n",
       "  'subplots': 4760,\n",
       "  'kiloton': 52073,\n",
       "  'chandon': 23381,\n",
       "  \"effect'\": 34713,\n",
       "  'snugly': 27637,\n",
       "  'kuei': 40856,\n",
       "  'welcomed': 9092,\n",
       "  'dishonor': 30071,\n",
       "  'concurrence': 52075,\n",
       "  'stoicism': 23382,\n",
       "  \"guys'\": 14896,\n",
       "  \"beroemd'\": 52077,\n",
       "  'butcher': 6703,\n",
       "  \"melfi's\": 40857,\n",
       "  'aargh': 30623,\n",
       "  'playhouse': 20599,\n",
       "  'wickedly': 11308,\n",
       "  'fit': 1180,\n",
       "  'labratory': 52078,\n",
       "  'lifeline': 40859,\n",
       "  'screaming': 1927,\n",
       "  'fix': 4287,\n",
       "  'cineliterate': 52079,\n",
       "  'fic': 52080,\n",
       "  'fia': 52081,\n",
       "  'fig': 34714,\n",
       "  'fmvs': 52082,\n",
       "  'fie': 52083,\n",
       "  'reentered': 52084,\n",
       "  'fin': 30574,\n",
       "  'doctresses': 52085,\n",
       "  'fil': 52086,\n",
       "  'zucker': 12606,\n",
       "  'ached': 31931,\n",
       "  'counsil': 52088,\n",
       "  'paterfamilias': 52089,\n",
       "  'songwriter': 13885,\n",
       "  'shivam': 34715,\n",
       "  'hurting': 9654,\n",
       "  'effects': 299,\n",
       "  'slauther': 52090,\n",
       "  \"'flame'\": 52091,\n",
       "  'sommerset': 52092,\n",
       "  'interwhined': 52093,\n",
       "  'whacking': 27638,\n",
       "  'bartok': 52094,\n",
       "  'barton': 8775,\n",
       "  'frewer': 21909,\n",
       "  \"fi'\": 52095,\n",
       "  'ingrid': 6192,\n",
       "  'stribor': 30575,\n",
       "  'approporiately': 52096,\n",
       "  'wobblyhand': 52097,\n",
       "  'tantalisingly': 52098,\n",
       "  'ankylosaurus': 52099,\n",
       "  'parasites': 17634,\n",
       "  'childen': 52100,\n",
       "  \"jenkins'\": 52101,\n",
       "  'metafiction': 52102,\n",
       "  'golem': 17635,\n",
       "  'indiscretion': 40860,\n",
       "  \"reeves'\": 23383,\n",
       "  \"inamorata's\": 57781,\n",
       "  'brittannica': 52104,\n",
       "  'adapt': 7916,\n",
       "  \"russo's\": 30576,\n",
       "  'guitarists': 48246,\n",
       "  'abbott': 10553,\n",
       "  'abbots': 40861,\n",
       "  'lanisha': 17649,\n",
       "  'magickal': 40863,\n",
       "  'mattter': 52105,\n",
       "  \"'willy\": 52106,\n",
       "  'pumpkins': 34716,\n",
       "  'stuntpeople': 52107,\n",
       "  'estimate': 30577,\n",
       "  'ugghhh': 40864,\n",
       "  'gameplay': 11309,\n",
       "  \"wern't\": 52108,\n",
       "  \"n'sync\": 40865,\n",
       "  'sickeningly': 16117,\n",
       "  'chiara': 40866,\n",
       "  'disturbed': 4011,\n",
       "  'portmanteau': 40867,\n",
       "  'ineffectively': 52109,\n",
       "  \"duchonvey's\": 82143,\n",
       "  \"nasty'\": 37519,\n",
       "  'purpose': 1285,\n",
       "  'lazers': 52112,\n",
       "  'lightened': 28105,\n",
       "  'kaliganj': 52113,\n",
       "  'popularism': 52114,\n",
       "  \"damme's\": 18511,\n",
       "  'stylistics': 30578,\n",
       "  'mindgaming': 52115,\n",
       "  'spoilerish': 46449,\n",
       "  \"'corny'\": 52117,\n",
       "  'boerner': 34718,\n",
       "  'olds': 6792,\n",
       "  'bakelite': 52118,\n",
       "  'renovated': 27639,\n",
       "  'forrester': 27640,\n",
       "  \"lumiere's\": 52119,\n",
       "  'gaskets': 52024,\n",
       "  'needed': 884,\n",
       "  'smight': 34719,\n",
       "  'master': 1297,\n",
       "  \"edie's\": 25905,\n",
       "  'seeber': 40868,\n",
       "  'hiya': 52120,\n",
       "  'fuzziness': 52121,\n",
       "  'genesis': 14897,\n",
       "  'rewards': 12607,\n",
       "  'enthrall': 30579,\n",
       "  \"'about\": 40869,\n",
       "  \"recollection's\": 52122,\n",
       "  'mutilated': 11039,\n",
       "  'fatherlands': 52123,\n",
       "  \"fischer's\": 52124,\n",
       "  'positively': 5399,\n",
       "  '270': 34705,\n",
       "  'ahmed': 34720,\n",
       "  'zatoichi': 9836,\n",
       "  'bannister': 13886,\n",
       "  'anniversaries': 52127,\n",
       "  \"helm's\": 30580,\n",
       "  \"'work'\": 52128,\n",
       "  'exclaimed': 34721,\n",
       "  \"'unfunny'\": 52129,\n",
       "  '274': 52029,\n",
       "  'feeling': 544,\n",
       "  \"wanda's\": 52131,\n",
       "  'dolan': 33266,\n",
       "  '278': 52133,\n",
       "  'peacoat': 52134,\n",
       "  'brawny': 40870,\n",
       "  'mishra': 40871,\n",
       "  'worlders': 40872,\n",
       "  'protags': 52135,\n",
       "  'skullcap': 52136,\n",
       "  'dastagir': 57596,\n",
       "  'affairs': 5622,\n",
       "  'wholesome': 7799,\n",
       "  'hymen': 52137,\n",
       "  'paramedics': 25246,\n",
       "  'unpersons': 52138,\n",
       "  'heavyarms': 52139,\n",
       "  'affaire': 52140,\n",
       "  'coulisses': 52141,\n",
       "  'hymer': 40873,\n",
       "  'kremlin': 52142,\n",
       "  'shipments': 30581,\n",
       "  'pixilated': 52143,\n",
       "  \"'00s\": 30582,\n",
       "  'diminishing': 18512,\n",
       "  'cinematic': 1357,\n",
       "  'resonates': 14898,\n",
       "  'simplify': 40874,\n",
       "  \"nature'\": 40875,\n",
       "  'temptresses': 40876,\n",
       "  'reverence': 16822,\n",
       "  'resonated': 19502,\n",
       "  'dailey': 34722,\n",
       "  '2\\x85': 52144,\n",
       "  'treize': 27641,\n",
       "  'majo': 52145,\n",
       "  'kiya': 21910,\n",
       "  'woolnough': 52146,\n",
       "  'thanatos': 39797,\n",
       "  'sandoval': 35731,\n",
       "  'dorama': 40879,\n",
       "  \"o'shaughnessy\": 52147,\n",
       "  'tech': 4988,\n",
       "  'fugitives': 32018,\n",
       "  'teck': 30583,\n",
       "  \"'e'\": 76125,\n",
       "  'doesn’t': 40881,\n",
       "  'purged': 52149,\n",
       "  'saying': 657,\n",
       "  \"martians'\": 41095,\n",
       "  'norliss': 23418,\n",
       "  'dickey': 27642,\n",
       "  'dicker': 52152,\n",
       "  \"'sependipity\": 52153,\n",
       "  'padded': 8422,\n",
       "  'ordell': 57792,\n",
       "  \"sturges'\": 40882,\n",
       "  'independentcritics': 52154,\n",
       "  'tempted': 5745,\n",
       "  \"atkinson's\": 34724,\n",
       "  'hounded': 25247,\n",
       "  'apace': 52155,\n",
       "  'clicked': 15494,\n",
       "  \"'humor'\": 30584,\n",
       "  \"martino's\": 17177,\n",
       "  \"'supporting\": 52156,\n",
       "  'warmongering': 52032,\n",
       "  \"zemeckis's\": 34725,\n",
       "  'lube': 21911,\n",
       "  'shocky': 52157,\n",
       "  'plate': 7476,\n",
       "  'plata': 40883,\n",
       "  'sturgess': 40884,\n",
       "  \"nerds'\": 40885,\n",
       "  'plato': 20600,\n",
       "  'plath': 34726,\n",
       "  'platt': 40886,\n",
       "  'mcnab': 52159,\n",
       "  'clumsiness': 27643,\n",
       "  'altogether': 3899,\n",
       "  'massacring': 42584,\n",
       "  'bicenntinial': 52160,\n",
       "  'skaal': 40887,\n",
       "  'droning': 14360,\n",
       "  'lds': 8776,\n",
       "  'jaguar': 21912,\n",
       "  \"cale's\": 34727,\n",
       "  'nicely': 1777,\n",
       "  'mummy': 4588,\n",
       "  \"lot's\": 18513,\n",
       "  'patch': 10086,\n",
       "  'kerkhof': 50202,\n",
       "  \"leader's\": 52161,\n",
       "  \"'movie\": 27644,\n",
       "  'uncomfirmed': 52162,\n",
       "  'heirloom': 40888,\n",
       "  'wrangle': 47360,\n",
       "  'emotion\\x85': 52163,\n",
       "  \"'stargate'\": 52164,\n",
       "  'pinoy': 40889,\n",
       "  'conchatta': 40890,\n",
       "  'broeke': 41128,\n",
       "  'advisedly': 40891,\n",
       "  \"barker's\": 17636,\n",
       "  'descours': 52166,\n",
       "  'lots': 772,\n",
       "  'lotr': 9259,\n",
       "  'irs': 9879,\n",
       "  'lott': 52167,\n",
       "  'xvi': 40892,\n",
       "  'irk': 34728,\n",
       "  'irl': 52168,\n",
       "  'ira': 6887,\n",
       "  'belzer': 21913,\n",
       "  'irc': 52169,\n",
       "  'ire': 27645,\n",
       "  'requisites': 40893,\n",
       "  'discipline': 7693,\n",
       "  'lyoko': 52961,\n",
       "  'extend': 11310,\n",
       "  'nature': 873,\n",
       "  \"'dickie'\": 52170,\n",
       "  'optimist': 40894,\n",
       "  'lapping': 30586,\n",
       "  'superficial': 3900,\n",
       "  'vestment': 52171,\n",
       "  'extent': 2823,\n",
       "  'tendons': 52172,\n",
       "  \"heller's\": 52173,\n",
       "  'quagmires': 52174,\n",
       "  'miyako': 52175,\n",
       "  'moocow': 20601,\n",
       "  \"coles'\": 52176,\n",
       "  'lookit': 40895,\n",
       "  'ravenously': 52177,\n",
       "  'levitating': 40896,\n",
       "  'perfunctorily': 52178,\n",
       "  'lookin': 30587,\n",
       "  \"lot'\": 40898,\n",
       "  'lookie': 52179,\n",
       "  'fearlessly': 34870,\n",
       "  'libyan': 52181,\n",
       "  'fondles': 40899,\n",
       "  'gopher': 35714,\n",
       "  'wearying': 40901,\n",
       "  \"nz's\": 52182,\n",
       "  'minuses': 27646,\n",
       "  'puposelessly': 52183,\n",
       "  'shandling': 52184,\n",
       "  'decapitates': 31268,\n",
       "  'humming': 11929,\n",
       "  \"'nother\": 40902,\n",
       "  'smackdown': 21914,\n",
       "  'underdone': 30588,\n",
       "  'frf': 40903,\n",
       "  'triviality': 52185,\n",
       "  'fro': 25248,\n",
       "  'bothers': 8777,\n",
       "  \"'kensington\": 52186,\n",
       "  'much': 73,\n",
       "  'muco': 34730,\n",
       "  'wiseguy': 22615,\n",
       "  \"richie's\": 27648,\n",
       "  'tonino': 40904,\n",
       "  'unleavened': 52187,\n",
       "  'fry': 11587,\n",
       "  \"'tv'\": 40905,\n",
       "  'toning': 40906,\n",
       "  'obese': 14361,\n",
       "  'sensationalized': 30589,\n",
       "  'spiv': 40907,\n",
       "  'spit': 6259,\n",
       "  'arkin': 7364,\n",
       "  'charleton': 21915,\n",
       "  'jeon': 16823,\n",
       "  'boardroom': 21916,\n",
       "  'doubts': 4989,\n",
       "  'spin': 3084,\n",
       "  'hepo': 53083,\n",
       "  'wildcat': 27649,\n",
       "  'venoms': 10584,\n",
       "  'misconstrues': 52191,\n",
       "  'mesmerising': 18514,\n",
       "  'misconstrued': 40908,\n",
       "  'rescinds': 52192,\n",
       "  'prostrate': 52193,\n",
       "  'majid': 40909,\n",
       "  'climbed': 16479,\n",
       "  'canoeing': 34731,\n",
       "  'majin': 52195,\n",
       "  'animie': 57804,\n",
       "  'sylke': 40910,\n",
       "  'conditioned': 14899,\n",
       "  'waddell': 40911,\n",
       "  '3\\x85': 52196,\n",
       "  'hyperdrive': 41188,\n",
       "  'conditioner': 34732,\n",
       "  'bricklayer': 53153,\n",
       "  'hong': 2576,\n",
       "  'memoriam': 52198,\n",
       "  'inventively': 30592,\n",
       "  \"levant's\": 25249,\n",
       "  'portobello': 20638,\n",
       "  'remand': 52200,\n",
       "  'mummified': 19504,\n",
       "  'honk': 27650,\n",
       "  'spews': 19505,\n",
       "  'visitations': 40912,\n",
       "  'mummifies': 52201,\n",
       "  'cavanaugh': 25250,\n",
       "  'zeon': 23385,\n",
       "  \"jungle's\": 40913,\n",
       "  'viertel': 34733,\n",
       "  'frenchmen': 27651,\n",
       "  'torpedoes': 52202,\n",
       "  'schlessinger': 52203,\n",
       "  'torpedoed': 34734,\n",
       "  'blister': 69876,\n",
       "  'cinefest': 52204,\n",
       "  'furlough': 34735,\n",
       "  'mainsequence': 52205,\n",
       "  'mentors': 40914,\n",
       "  'academic': 9094,\n",
       "  'stillness': 20602,\n",
       "  'academia': 40915,\n",
       "  'lonelier': 52206,\n",
       "  'nibby': 52207,\n",
       "  \"losers'\": 52208,\n",
       "  'cineastes': 40916,\n",
       "  'corporate': 4449,\n",
       "  'massaging': 40917,\n",
       "  'bellow': 30593,\n",
       "  'absurdities': 19506,\n",
       "  'expetations': 53241,\n",
       "  'nyfiken': 40918,\n",
       "  'mehras': 75638,\n",
       "  'lasse': 52209,\n",
       "  'visability': 52210,\n",
       "  'militarily': 33946,\n",
       "  \"elder'\": 52211,\n",
       "  'gainsbourg': 19023,\n",
       "  'hah': 20603,\n",
       "  'hai': 13420,\n",
       "  'haj': 34736,\n",
       "  'hak': 25251,\n",
       "  'hal': 4311,\n",
       "  'ham': 4892,\n",
       "  'duffer': 53259,\n",
       "  'haa': 52213,\n",
       "  'had': 66,\n",
       "  'advancement': 11930,\n",
       "  'hag': 16825,\n",
       "  \"hand'\": 25252,\n",
       "  'hay': 13421,\n",
       "  'mcnamara': 20604,\n",
       "  \"mozart's\": 52214,\n",
       "  'duffel': 30731,\n",
       "  'haq': 30594,\n",
       "  'har': 13887,\n",
       "  'has': 44,\n",
       "  'hat': 2401,\n",
       "  'hav': 40919,\n",
       "  'haw': 30595,\n",
       "  'figtings': 52215,\n",
       "  'elders': 15495,\n",
       "  'underpanted': 52216,\n",
       "  'pninson': 52217,\n",
       "  'unequivocally': 27652,\n",
       "  \"barbara's\": 23673,\n",
       "  \"bello'\": 52219,\n",
       "  'indicative': 12997,\n",
       "  'yawnfest': 40920,\n",
       "  'hexploitation': 52220,\n",
       "  \"loder's\": 52221,\n",
       "  'sleuthing': 27653,\n",
       "  \"justin's\": 32622,\n",
       "  \"'ball\": 52222,\n",
       "  \"'summer\": 52223,\n",
       "  \"'demons'\": 34935,\n",
       "  \"mormon's\": 52225,\n",
       "  \"laughton's\": 34737,\n",
       "  'debell': 52226,\n",
       "  'shipyard': 39724,\n",
       "  'unabashedly': 30597,\n",
       "  'disks': 40401,\n",
       "  'crowd': 2290,\n",
       "  'crowe': 10087,\n",
       "  \"vancouver's\": 56434,\n",
       "  'mosques': 34738,\n",
       "  'crown': 6627,\n",
       "  'culpas': 52227,\n",
       "  'crows': 27654,\n",
       "  'surrell': 53344,\n",
       "  'flowless': 52229,\n",
       "  'sheirk': 52230,\n",
       "  \"'three\": 40923,\n",
       "  \"peterson'\": 52231,\n",
       "  'ooverall': 52232,\n",
       "  'perchance': 40924,\n",
       "  'bottom': 1321,\n",
       "  'chabert': 53363,\n",
       "  'sneha': 52233,\n",
       "  'inhuman': 13888,\n",
       "  'ichii': 52234,\n",
       "  'ursla': 52235,\n",
       "  'completly': 30598,\n",
       "  'moviedom': 40925,\n",
       "  'raddick': 52236,\n",
       "  'brundage': 51995,\n",
       "  'brigades': 40926,\n",
       "  'starring': 1181,\n",
       "  \"'goal'\": 52237,\n",
       "  'caskets': 52238,\n",
       "  'willcock': 52239,\n",
       "  \"threesome's\": 52240,\n",
       "  \"mosque'\": 52241,\n",
       "  \"cover's\": 52242,\n",
       "  'spaceships': 17637,\n",
       "  'anomalous': 40927,\n",
       "  'ptsd': 27655,\n",
       "  'shirdan': 52243,\n",
       "  'obscenity': 21962,\n",
       "  'lemmings': 30599,\n",
       "  'duccio': 30600,\n",
       "  \"levene's\": 52244,\n",
       "  \"'gorby'\": 52245,\n",
       "  \"teenager's\": 25255,\n",
       "  'marshall': 5340,\n",
       "  'honeymoon': 9095,\n",
       "  'shoots': 3231,\n",
       "  'despised': 12258,\n",
       "  'okabasho': 52246,\n",
       "  'fabric': 8289,\n",
       "  'cannavale': 18515,\n",
       "  'raped': 3537,\n",
       "  \"tutt's\": 52247,\n",
       "  'grasping': 17638,\n",
       "  'despises': 18516,\n",
       "  \"thief's\": 40928,\n",
       "  'rapes': 8926,\n",
       "  'raper': 52248,\n",
       "  \"eyre'\": 27656,\n",
       "  'walchek': 52249,\n",
       "  \"elmo's\": 23386,\n",
       "  'perfumes': 40929,\n",
       "  'spurting': 21918,\n",
       "  \"exposition'\\x85\": 52250,\n",
       "  'denoting': 52251,\n",
       "  'thesaurus': 34740,\n",
       "  \"shoot'\": 40930,\n",
       "  'bonejack': 49759,\n",
       "  'simpsonian': 52253,\n",
       "  'hebetude': 30601,\n",
       "  \"hallow's\": 34741,\n",
       "  'desperation\\x85': 52254,\n",
       "  'incinerator': 34742,\n",
       "  'congratulations': 10308,\n",
       "  'humbled': 52255,\n",
       "  \"else's\": 5924,\n",
       "  'trelkovski': 40845,\n",
       "  \"rape'\": 52256,\n",
       "  \"'chapters'\": 59386,\n",
       "  '1600s': 52257,\n",
       "  'martian': 7253,\n",
       "  'nicest': 25256,\n",
       "  'eyred': 52259,\n",
       "  'passenger': 9457,\n",
       "  'disgrace': 6041,\n",
       "  'moderne': 52260,\n",
       "  'barrymore': 5120,\n",
       "  'yankovich': 52261,\n",
       "  'moderns': 40931,\n",
       "  'studliest': 52262,\n",
       "  'bedsheet': 52263,\n",
       "  'decapitation': 14900,\n",
       "  'slurring': 52264,\n",
       "  \"'nunsploitation'\": 52265,\n",
       "  \"'character'\": 34743,\n",
       "  'cambodia': 9880,\n",
       "  'rebelious': 52266,\n",
       "  'pasadena': 27657,\n",
       "  'crowne': 40932,\n",
       "  \"'bedchamber\": 52267,\n",
       "  'conjectural': 52268,\n",
       "  'appologize': 52269,\n",
       "  'halfassing': 52270,\n",
       "  'paycheque': 57816,\n",
       "  'palms': 20606,\n",
       "  \"'islands\": 52271,\n",
       "  'hawked': 40933,\n",
       "  'palme': 21919,\n",
       "  'conservatively': 40934,\n",
       "  'larp': 64007,\n",
       "  'palma': 5558,\n",
       "  'smelling': 21920,\n",
       "  'aragorn': 12998,\n",
       "  'hawker': 52272,\n",
       "  'hawkes': 52273,\n",
       "  'explosions': 3975,\n",
       "  'loren': 8059,\n",
       "  \"pyle's\": 52274,\n",
       "  'shootout': 6704,\n",
       "  \"mike's\": 18517,\n",
       "  \"driscoll's\": 52275,\n",
       "  'cogsworth': 40935,\n",
       "  \"britian's\": 52276,\n",
       "  'childs': 34744,\n",
       "  \"portrait's\": 52277,\n",
       "  'chain': 3626,\n",
       "  'whoever': 2497,\n",
       "  'puttered': 52278,\n",
       "  'childe': 52279,\n",
       "  'maywether': 52280,\n",
       "  'chair': 3036,\n",
       "  \"rance's\": 52281,\n",
       "  'machu': 34745,\n",
       "  'ballet': 4517,\n",
       "  'grapples': 34746,\n",
       "  'summerize': 76152,\n",
       "  'freelance': 30603,\n",
       "  \"andrea's\": 52283,\n",
       "  '\\x91very': 52284,\n",
       "  'coolidge': 45879,\n",
       "  'mache': 18518,\n",
       "  'balled': 52285,\n",
       "  'grappled': 40937,\n",
       "  'macha': 18519,\n",
       "  'underlining': 21921,\n",
       "  'macho': 5623,\n",
       "  'oversight': 19507,\n",
       "  'machi': 25257,\n",
       "  'verbally': 11311,\n",
       "  'tenacious': 21922,\n",
       "  'windshields': 40938,\n",
       "  'paychecks': 18557,\n",
       "  'jerk': 3396,\n",
       "  \"good'\": 11931,\n",
       "  'prancer': 34748,\n",
       "  'prances': 21923,\n",
       "  'olympus': 52286,\n",
       "  'lark': 21924,\n",
       "  'embark': 10785,\n",
       "  'gloomy': 7365,\n",
       "  'jehaan': 52287,\n",
       "  'turaqui': 52288,\n",
       "  \"child'\": 20607,\n",
       "  'locked': 2894,\n",
       "  'pranced': 52289,\n",
       "  'exact': 2588,\n",
       "  'unattuned': 52290,\n",
       "  'minute': 783,\n",
       "  'skewed': 16118,\n",
       "  'hodgins': 40940,\n",
       "  'skewer': 34749,\n",
       "  'think\\x85': 52291,\n",
       "  'rosenstein': 38765,\n",
       "  'helmit': 52292,\n",
       "  'wrestlemanias': 34750,\n",
       "  'hindered': 16826,\n",
       "  \"martha's\": 30604,\n",
       "  'cheree': 52293,\n",
       "  \"pluckin'\": 52294,\n",
       "  'ogles': 40941,\n",
       "  'heavyweight': 11932,\n",
       "  'aada': 82190,\n",
       "  'chopping': 11312,\n",
       "  'strongboy': 61534,\n",
       "  'hegemonic': 41342,\n",
       "  'adorns': 40942,\n",
       "  'xxth': 41346,\n",
       "  'nobuhiro': 34751,\n",
       "  'capitães': 52298,\n",
       "  'kavogianni': 52299,\n",
       "  'antwerp': 13422,\n",
       "  'celebrated': 6538,\n",
       "  'roarke': 52300,\n",
       "  'baggins': 40943,\n",
       "  'cheeseburgers': 31270,\n",
       "  'matras': 52301,\n",
       "  \"nineties'\": 52302,\n",
       "  \"'craig'\": 52303,\n",
       "  'celebrates': 12999,\n",
       "  'unintentionally': 3383,\n",
       "  'drafted': 14362,\n",
       "  'climby': 52304,\n",
       "  '303': 52305,\n",
       "  'oldies': 18520,\n",
       "  'climbs': 9096,\n",
       "  'honour': 9655,\n",
       "  'plucking': 34752,\n",
       "  '305': 30074,\n",
       "  'address': 5514,\n",
       "  'menjou': 40944,\n",
       "  \"'freak'\": 42592,\n",
       "  'dwindling': 19508,\n",
       "  'benson': 9458,\n",
       "  'white’s': 52307,\n",
       "  'shamelessness': 40945,\n",
       "  'impacted': 21925,\n",
       "  'upatz': 52308,\n",
       "  'cusack': 3840,\n",
       "  \"flavia's\": 37567,\n",
       "  'effette': 52309,\n",
       "  'influx': 34753,\n",
       "  'boooooooo': 52310,\n",
       "  'dimitrova': 52311,\n",
       "  'houseman': 13423,\n",
       "  'bigas': 25259,\n",
       "  'boylen': 52312,\n",
       "  'phillipenes': 52313,\n",
       "  'fakery': 40946,\n",
       "  \"grandpa's\": 27658,\n",
       "  'darnell': 27659,\n",
       "  'undergone': 19509,\n",
       "  'handbags': 52315,\n",
       "  'perished': 21926,\n",
       "  'pooped': 37778,\n",
       "  'vigour': 27660,\n",
       "  'opposed': 3627,\n",
       "  'etude': 52316,\n",
       "  \"caine's\": 11799,\n",
       "  'doozers': 52317,\n",
       "  'photojournals': 34754,\n",
       "  'perishes': 52318,\n",
       "  'constrains': 34755,\n",
       "  'migenes': 40948,\n",
       "  'consoled': 30605,\n",
       "  'alastair': 16827,\n",
       "  'wvs': 52319,\n",
       "  'ooooooh': 52320,\n",
       "  'approving': 34756,\n",
       "  'consoles': 40949,\n",
       "  'disparagement': 52064,\n",
       "  'futureistic': 52322,\n",
       "  'rebounding': 52323,\n",
       "  \"'date\": 52324,\n",
       "  'gregoire': 52325,\n",
       "  'rutherford': 21927,\n",
       "  'americanised': 34757,\n",
       "  'novikov': 82196,\n",
       "  'following': 1042,\n",
       "  'munroe': 34758,\n",
       "  \"morita'\": 52326,\n",
       "  'christenssen': 52327,\n",
       "  'oatmeal': 23106,\n",
       "  'fossey': 25260,\n",
       "  'livered': 40950,\n",
       "  'listens': 13000,\n",
       "  \"'marci\": 76164,\n",
       "  \"otis's\": 52330,\n",
       "  'thanking': 23387,\n",
       "  'maude': 16019,\n",
       "  'extensions': 34759,\n",
       "  'ameteurish': 52332,\n",
       "  \"commender's\": 52333,\n",
       "  'agricultural': 27661,\n",
       "  'convincingly': 4518,\n",
       "  'fueled': 17639,\n",
       "  'mahattan': 54014,\n",
       "  \"paris's\": 40952,\n",
       "  'vulkan': 52336,\n",
       "  'stapes': 52337,\n",
       "  'odysessy': 52338,\n",
       "  'harmon': 12259,\n",
       "  'surfing': 4252,\n",
       "  'halloran': 23494,\n",
       "  'unbelieveably': 49580,\n",
       "  \"'offed'\": 52339,\n",
       "  'quadrant': 30607,\n",
       "  'inhabiting': 19510,\n",
       "  'nebbish': 34760,\n",
       "  'forebears': 40953,\n",
       "  'skirmish': 34761,\n",
       "  'ocassionally': 52340,\n",
       "  \"'resist\": 52341,\n",
       "  'impactful': 21928,\n",
       "  'spicier': 52342,\n",
       "  'touristy': 40954,\n",
       "  \"'football'\": 52343,\n",
       "  'webpage': 40955,\n",
       "  'exurbia': 52345,\n",
       "  'jucier': 52346,\n",
       "  'professors': 14901,\n",
       "  'structuring': 34762,\n",
       "  'jig': 30608,\n",
       "  'overlord': 40956,\n",
       "  'disconnect': 25261,\n",
       "  'sniffle': 82201,\n",
       "  'slimeball': 40957,\n",
       "  'jia': 40958,\n",
       "  'milked': 16828,\n",
       "  'banjoes': 40959,\n",
       "  'jim': 1237,\n",
       "  'workforces': 52348,\n",
       "  'jip': 52349,\n",
       "  'rotweiller': 52350,\n",
       "  'mundaneness': 34763,\n",
       "  \"'ninja'\": 52351,\n",
       "  \"dead'\": 11040,\n",
       "  \"cipriani's\": 40960,\n",
       "  'modestly': 20608,\n",
       "  \"professor'\": 52352,\n",
       "  'shacked': 40961,\n",
       "  'bashful': 34764,\n",
       "  'sorter': 23388,\n",
       "  'overpowering': 16120,\n",
       "  'workmanlike': 18521,\n",
       "  'henpecked': 27662,\n",
       "  'sorted': 18522,\n",
       "  \"jōb's\": 52354,\n",
       "  \"'always\": 52355,\n",
       "  \"'baptists\": 34765,\n",
       "  'dreamcatchers': 52356,\n",
       "  \"'silence'\": 52357,\n",
       "  'hickory': 21929,\n",
       "  'fun\\x97yet': 52358,\n",
       "  'breakumentary': 52359,\n",
       "  'didn': 15496,\n",
       "  'didi': 52360,\n",
       "  'pealing': 52361,\n",
       "  'dispite': 40962,\n",
       "  \"italy's\": 25262,\n",
       "  'instability': 21930,\n",
       "  'quarter': 6539,\n",
       "  'quartet': 12608,\n",
       "  'padmé': 52362,\n",
       "  \"'bleedmedry\": 52363,\n",
       "  'pahalniuk': 52364,\n",
       "  'honduras': 52365,\n",
       "  'bursting': 10786,\n",
       "  \"pablo's\": 41465,\n",
       "  'irremediably': 52367,\n",
       "  'presages': 40963,\n",
       "  'bowlegged': 57832,\n",
       "  'dalip': 65183,\n",
       "  'entering': 6260,\n",
       "  'newsradio': 76172,\n",
       "  'presaged': 54150,\n",
       "  \"giallo's\": 27663,\n",
       "  'bouyant': 40964,\n",
       "  'amerterish': 52368,\n",
       "  'rajni': 18523,\n",
       "  'leeves': 30610,\n",
       "  'macauley': 34767,\n",
       "  'seriously': 612,\n",
       "  'sugercoma': 52369,\n",
       "  'grimstead': 52370,\n",
       "  \"'fairy'\": 52371,\n",
       "  'zenda': 30611,\n",
       "  \"'twins'\": 52372,\n",
       "  'realisation': 17640,\n",
       "  'highsmith': 27664,\n",
       "  'raunchy': 7817,\n",
       "  'incentives': 40965,\n",
       "  'flatson': 52374,\n",
       "  'snooker': 35097,\n",
       "  'crazies': 16829,\n",
       "  'crazier': 14902,\n",
       "  'grandma': 7094,\n",
       "  'napunsaktha': 52375,\n",
       "  'workmanship': 30612,\n",
       "  'reisner': 52376,\n",
       "  \"sanford's\": 61306,\n",
       "  '\\x91doña': 52377,\n",
       "  'modest': 6108,\n",
       "  \"everything's\": 19153,\n",
       "  'hamer': 40966,\n",
       "  \"couldn't'\": 52379,\n",
       "  'quibble': 13001,\n",
       "  'socking': 52380,\n",
       "  'tingler': 21931,\n",
       "  'gutman': 52381,\n",
       "  'lachlan': 40967,\n",
       "  'tableaus': 52382,\n",
       "  'headbanger': 52383,\n",
       "  'spoken': 2847,\n",
       "  'cerebrally': 34768,\n",
       "  \"'road\": 23490,\n",
       "  'tableaux': 21932,\n",
       "  \"proust's\": 40968,\n",
       "  'periodical': 40969,\n",
       "  \"shoveller's\": 52385,\n",
       "  'tamara': 25263,\n",
       "  'affords': 17641,\n",
       "  'concert': 3249,\n",
       "  \"yara's\": 87955,\n",
       "  'someome': 52386,\n",
       "  'lingering': 8424,\n",
       "  \"abraham's\": 41511,\n",
       "  'beesley': 34769,\n",
       "  'cherbourg': 34770,\n",
       "  'kagan': 28624,\n",
       "  'snatch': 9097,\n",
       "  \"miyazaki's\": 9260,\n",
       "  'absorbs': 25264,\n",
       "  \"koltai's\": 40970,\n",
       "  'tingled': 64027,\n",
       "  'crossroads': 19511,\n",
       "  'rehab': 16121,\n",
       "  'falworth': 52389,\n",
       "  'sequals': 52390,\n",
       "  ...})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx),idx['the'],idx['and'],idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}\n",
    "idx2word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the reviews using code copied from keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n",
      "65560576/65552540 [==============================] - 730s 11us/step\n"
     ]
    }
   ],
   "source": [
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, 25000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train),type(labels_train),len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 433, 39)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[:1]),len(x_train[1]),x_train[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word of the first review is 23022. Let's see what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[23022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the whole review, mapped from ids to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are 1 for positive, 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 25000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn),len(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'map' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1cbba8cb9297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'map' and 'int'"
     ]
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dayou/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 128s 5ms/step - loss: 0.4621 - acc: 0.7558 - val_loss: 0.2938 - val_acc: 0.8810\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 129s 5ms/step - loss: 0.1925 - acc: 0.9286 - val_loss: 0.3148 - val_acc: 0.8715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7064ce5940>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. So we're short of that, but on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n",
    "    Dropout(0.2),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 4s - loss: 0.4984 - acc: 0.7250 - val_loss: 0.2922 - val_acc: 0.8816\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 4s - loss: 0.2971 - acc: 0.8836 - val_loss: 0.2681 - val_acc: 0.8911\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 4s - loss: 0.2568 - acc: 0.8983 - val_loss: 0.2551 - val_acc: 0.8947\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 4s - loss: 0.2427 - acc: 0.9029 - val_loss: 0.2558 - val_acc: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99cfa785d0>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's well past the Stanford paper's accuracy - another win for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3b424bf3dbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_glove_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6B.50d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-3e28a11ae58c>\u001b[0m in \u001b[0;36mload_vectors\u001b[0;34m(loc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     return (load_array(loc+'.dat'),\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_words.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         pickle.load(open(loc+'_idx.pkl','rb')))\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 4s - loss: 0.5217 - acc: 0.7172 - val_loss: 0.2942 - val_acc: 0.8815\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 4s - loss: 0.3169 - acc: 0.8719 - val_loss: 0.2662 - val_acc: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0de0f2d910>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 4s - loss: 0.2751 - acc: 0.8911 - val_loss: 0.2500 - val_acc: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0de0c4e0d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As expected, that's given us a nice little boost. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-size CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' [excellent blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the functional API to create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.3997 - acc: 0.8207 - val_loss: 0.3032 - val_acc: 0.8943\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2882 - acc: 0.8832 - val_loss: 0.2646 - val_acc: 0.9029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b79b7990>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interestingly, I found that in this case I got best results when I started the embedding layer as being trainable, and then set it to non-trainable after a couple of epochs. I have no idea why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2556 - acc: 0.8949 - val_loss: 0.2534 - val_acc: 0.9024\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2360 - acc: 0.9057 - val_loss: 0.2577 - val_acc: 0.9036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b74de110>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This more complex architecture has given us another boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't covered this bit yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_13 (Embedding)         (None, 500, 32)       160064      embedding_input_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                   (None, 100)           53200       embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             101         lstm_13[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 213365\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n",
    "              W_regularizer=l2(1e-6), dropout=0.2),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 100s - loss: 0.5007 - acc: 0.7446 - val_loss: 0.3475 - val_acc: 0.8531\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 100s - loss: 0.3524 - acc: 0.8507 - val_loss: 0.3602 - val_acc: 0.8453\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.3750 - acc: 0.8342 - val_loss: 0.4758 - val_acc: 0.7710\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.3238 - acc: 0.8652 - val_loss: 0.3094 - val_acc: 0.8725\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.2681 - acc: 0.8920 - val_loss: 0.3018 - val_acc: 0.8776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a16b12c50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
